{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "appliances = {\n",
    "        0: {'app': 'kettle', 'power': 2000},\n",
    "        # 1: {'app': 'toaster', 'power': 1500},\n",
    "        # 2: {'app': 'dish washer', 'power': 2200},\n",
    "        # 3: {'app': 'microwave', 'power':1500}\n",
    "        # 4: {'app': 'hair dryer', 'power': 1000},\n",
    "        # 5: {'app': 'audio amplifier', 'power': 20},\n",
    "    }\n",
    "\n",
    "#air dryer nao tem na casa 2?\n",
    "#tirar hair dryer e audio amplifier!!!\n",
    "#falta testar onde fica o dropout da ann!!\n",
    "\n",
    "deep_learning_technique = ['ANN']\n",
    "# deep_learning_technique = ['ANN', 'CNN', 'GRU']\n",
    "\n",
    "homes = [1,2,5]\n",
    "\n",
    "#esse dict soh serve pra dar load na rede\n",
    "app_to_image_size = {\n",
    "    'kettle': 48,\n",
    "    'toaster': 64,\n",
    "    'audio amplifier': 0,\n",
    "    'dish washer': 166,\n",
    "    'hair dryer': 27,\n",
    "    'microwave': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import nilmtk\n",
    "from nilmtk import DataSet, MeterGroup\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "sns.set()\n",
    "plt.style.use('ggplot')\n",
    "rcParams['figure.figsize'] = (20, 8)\n",
    "caminho_ukdale = '../ukdale.h5'\n",
    "\n",
    "# Lendo Dataset do Ukdale\n",
    "ukdale = DataSet(caminho_ukdale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"\n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, image_size, target_cols):\n",
    "        self.data = torch.from_numpy(data).to(device)\n",
    "        self.image_size = image_size\n",
    "        self.target_cols = target_cols\n",
    "        self.shape = self.__getshape__()\n",
    "        self.size = self.__getsize__()\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.image_size, 0:self.target_cols]\n",
    "        y = self.data[index:index+self.image_size, self.target_cols]\n",
    "        return x, y\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getshape__(self):\n",
    "        return (self.__len__(), *self.__getitem__(0)[0].shape)\n",
    "    \n",
    "    def __getsize__(self):\n",
    "        return (self.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(ukdale, appliance, power, house):\n",
    "    # Lendo eletrodomesticos da Casa House\n",
    "    elec = ukdale.buildings[house].elec\n",
    "\n",
    "    #Criando dataset do mains\n",
    "    main_df = next(elec.mains().load())\n",
    "    # print(main_df.head())\n",
    "    main_df.columns = [' '.join(col).strip() for col in main_df.columns.values]\n",
    "    main_df = main_df.rename(columns={'power apparent': 'pwr_app_mains', 'power active': 'pwr_act_mains', 'voltage': 'voltage_mains'})\n",
    "    main_df = main_df.sort_index()\n",
    "    \n",
    "    appliance_df = next(elec[appliance].load())\n",
    "    appliance_df.columns = ['pwr_act_' + appliance]\n",
    "    appliance_df = appliance_df.sort_index()\n",
    "    \n",
    "    #-------------------Upsample do main_df--------------------------#\n",
    "    appliance_df = appliance_df[appliance_df.index >= main_df.index[0]]\n",
    "\n",
    "    main_df = main_df.resample('6S', offset='5s').mean().interpolate()\n",
    "    main_df = main_df[main_df.index >= main_df.index[1]]\n",
    "    main_df = main_df[main_df.index <= appliance_df.index[-1]]\n",
    "\n",
    "    #------------------Resampling appliance_df-----------------------#\n",
    "    appliance_df = appliance_df.resample('6S', offset='5s').mean().interpolate()\n",
    "\n",
    "    #-------------------Merging dataframes---------------------------#\n",
    "    merged_df = main_df.merge(appliance_df, left_index=True, right_index=True)\n",
    "    merged_df['time'] = merged_df.index\n",
    "    merged_df['time'] = merged_df['time'].apply(lambda x: x.tz_localize(None))\n",
    "    merged_df.index = merged_df['time']\n",
    "    merged_df['dia_semana'] = merged_df.time.dt.dayofweek\n",
    "    merged_df['mes'] = merged_df.time.dt.month\n",
    "    merged_df['hora'] = merged_df.time.dt.hour\n",
    "    merged_df = merged_df.drop(columns=['time'])\n",
    "\n",
    "    #---------------Criando coluna classificatoria-------------------#\n",
    "    merged_df['ligado'] = merged_df['pwr_act_' + appliance] > power\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_training_from_test(df):\n",
    "    #Fazendo a divisao entre treino e teste de forma que 1 milhao de pontos sejam utilizados para treinar\n",
    "    train_test_divide = df.index[int(0.8*len(df))]\n",
    "\n",
    "    print(\"inicio treino: \" + str(df.index[0]))\n",
    "    print(\"final treino: \" + str(train_test_divide))\n",
    "    print(\"final teste: \" + str(df.index[-1]))\n",
    "    \n",
    "    train = df[df.index <= train_test_divide]\n",
    "    test = df[df.index > train_test_divide]\n",
    "    \n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_size(df):\n",
    "    df[\"ligado\"] = df[\"ligado\"].astype(int)\n",
    "    df_ligado = df[df['ligado'] == True][['ligado']].reset_index()\n",
    "    df_ligado['diferenca'] = df_ligado['time'].diff()\n",
    "    df_ligado[df_ligado['diferenca'] > '00:00:30']\n",
    "    \n",
    "    \n",
    "    # add a column saying if a row belongs to the same state as the one before it\n",
    "    df_ligado[\"is_first\"] = df_ligado['diferenca'] > \"00:00:06\"\n",
    "\n",
    "    # the cumulative sum - each \"clump\" gets its own integer id\n",
    "    df_ligado[\"value_group\"] = df_ligado[\"is_first\"].cumsum()\n",
    "\n",
    "    # get the rows corresponding to states beginning\n",
    "    start = df_ligado.groupby(\"value_group\", as_index=False).nth(0)\n",
    "    # get the rows corresponding to states ending\n",
    "    end = df_ligado.groupby(\"value_group\", as_index=False).nth(-1)\n",
    "\n",
    "    start_end = pd.DataFrame(\n",
    "        {\n",
    "            \"start\": start.index,\n",
    "            # add freq to get when the state ended\n",
    "            \"end\": end.index,\n",
    "        }\n",
    "    )\n",
    "    # convert timedeltas to seconds (float)\n",
    "    start_end[\"duration\"] = (\n",
    "        (start_end[\"end\"] - start_end[\"start\"]).apply(np.int64)\n",
    "    )\n",
    "\n",
    "    biggest_window = max(start_end.duration)\n",
    "    return biggest_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequential_dataset(df,  image_size, batch_size, target_cols):\n",
    "    df = df[['voltage_mains', 'pwr_app_mains', 'pwr_act_mains', 'dia_semana', 'hora', 'ligado']]\n",
    "    df_scaled = pd.DataFrame(np.append(\n",
    "        preprocessing.scale(df[['voltage_mains', 'pwr_app_mains', 'pwr_act_mains', 'dia_semana', 'hora']]), df[['ligado']].values, axis=1),\n",
    "                                index=df.index, columns=df.columns)\n",
    "    dataset = MyDataset(df_scaled.values, image_size, target_cols)\n",
    "    dfloader = torch.utils.data.DataLoader(dataset, batch_size = batch_size)\n",
    "    return dfloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_net(dp_tec,  image_size, batch_size):\n",
    "    n=5\n",
    "    if dp_tec == 'ANN':\n",
    "        class Net(nn.Module):    \n",
    "            def __init__(self):\n",
    "                super(Net, self).__init__()\n",
    "                self.hd1 = nn.Linear(image_size*n, 500*n)\n",
    "                self.hd2 = nn.Linear(500*n, 500*n)\n",
    "                self.hd3 = nn.Linear(500*n, 500*n)\n",
    "                self.hd4 = nn.Linear(500*n, image_size)\n",
    "                self.bn = nn.BatchNorm1d(500*n)\n",
    "                self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.hd1(x.reshape(x.size(0), -1)))\n",
    "                x = F.relu(self.bn(self.hd2(x)))\n",
    "        #         x = self.bn(x) aqui foi o melhor\n",
    "                # x = self.dropout(x) tava aqui\n",
    "                x = F.relu(self.hd3(x))\n",
    "                x = self.hd4(x)\n",
    "                x = self.bn(x)\n",
    "                x = self.dropout(x)\n",
    "                return x\n",
    "\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "        net = Net()\n",
    "\n",
    "    if dp_tec == 'CNN':            \n",
    "        class Net(nn.Module):    \n",
    "            def __init__(self):\n",
    "                super(Net, self).__init__()\n",
    "                self.pool1 = nn.MaxPool1d(5,1)\n",
    "                self.cv1 = nn.Conv1d(n, n*8, 3)\n",
    "                self.cv2 = nn.Conv1d(n*8, n*8, 3)\n",
    "                self.cv3 = nn.Conv1d(n*8, 16*n, 3)\n",
    "                self.cv4 = nn.Conv1d(16*n, 16*n, 3)\n",
    "                self.cv5 = nn.Conv1d(16*n, 32*n, 3)       \n",
    "                self.cv6 = nn.Conv1d(32*n, 32*n, 3)\n",
    "                self.cv7 = nn.Conv1d(32*n, 64*n, 3)\n",
    "                self.cv8 = nn.Conv1d(64*n, 64*n, 3)\n",
    "                self.fc1 = nn.Linear(64*n*(image_size - 40), 300)\n",
    "                self.fc2 = nn.Linear(300, image_size)\n",
    "                self.bn = nn.BatchNorm1d(64*n)\n",
    "                self.dropout = nn.Dropout(p=0.75)\n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.cv2(F.relu(self.cv2(F.relu(self.cv1(x))))))\n",
    "                x = self.pool1(x)\n",
    "                x = F.relu(self.cv4(F.relu(self.cv4(F.relu(self.cv3(x))))))\n",
    "                x = self.pool1(x)\n",
    "                x = F.relu(self.cv6(F.relu(self.cv6(F.relu(self.cv5(x))))))\n",
    "                x = self.pool1(x)\n",
    "                x = F.relu(self.cv8(F.relu(self.cv8(F.relu(self.cv7(x))))))\n",
    "                x = self.bn(x)\n",
    "                x = self.pool1(x)\n",
    "                x = F.relu(self.fc1(x.reshape(x.size(0), -1)))\n",
    "                x = self.dropout(x)\n",
    "                x = self.fc2(x)\n",
    "                return x\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Conv1d or type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01) \n",
    "        net = Net()\n",
    "\n",
    "\n",
    "    if dp_tec == 'GRU':\n",
    "        class EncoderRNN(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(EncoderRNN, self).__init__()\n",
    "                self.cv1 = nn.Conv1d(n, n*8, 3)\n",
    "                self.gru1 = nn.GRU(40, 128, num_layers = 2, bidirectional = True, batch_first = True)\n",
    "                self.fc1 = nn.Linear(256, 100)\n",
    "                self.fc2 = nn.Linear(100, image_size)\n",
    "                self.bn = nn.BatchNorm1d(image_size-2)\n",
    "                self.dropout = nn.Dropout(p=0.6)\n",
    "            def forward(self, x, hidden):\n",
    "                x = torch.transpose(x, 1, 2)\n",
    "                x = F.relu(self.cv1(x))\n",
    "                x = torch.transpose(x, 1, 2)        \n",
    "                x, hidden = self.gru1(x, hidden)\n",
    "                x = self.bn(x)\n",
    "                x = F.relu(self.fc1(F.relu(x[:,-1])))\n",
    "                x = self.dropout(x)\n",
    "                x = self.fc2(x)\n",
    "                return x, hidden\n",
    "            def initHidden(self):\n",
    "                return torch.zeros(4, batch_size, 128, device=device)\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Conv1d or type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "            if type(m) == nn.GRU:\n",
    "                for layer_p in m._all_weights:\n",
    "                    for p in layer_p:\n",
    "                        if 'weight' in p:\n",
    "                            #print(p, m.__getattr__(p))\n",
    "                            torch.nn.init.xavier_uniform_(m.__getattr__(p))\n",
    "                        if 'bias' in p:\n",
    "                            m.__getattr__(p).data.fill_(0.01)\n",
    "        def repackage_hidden(h):\n",
    "            \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "            if isinstance(h, torch.Tensor):\n",
    "                return h.detach()\n",
    "            else:\n",
    "                return tuple(repackage_hidden(v) for v in h)\n",
    "        net = EncoderRNN()\n",
    "\n",
    "    #inicializacao\n",
    "    net = net.to(device)\n",
    "    net.apply(init_weights)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    net.zero_grad()\n",
    "    \n",
    "    return net, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, trainloader, optimizer, criterion, appliance, dp_tec, image_size):\n",
    "    comeco = datetime.datetime.now()\n",
    "    print('###########################')\n",
    "    print('Comeco do treino para ', appliance, ' utilizando a rede ', dp_tec, ' inicio as ', comeco)\n",
    "    if dp_tec == 'GRU':\n",
    "        net.train()\n",
    "    # for epoch in range(1):\n",
    "    for epoch in range(5):  # loop over the dataset multiple times\n",
    "        hidden = False\n",
    "        real_i = 0\n",
    "        running_loss = 0.0\n",
    "        if dp_tec == 'GRU':\n",
    "            hidden = net.initHidden()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            if i > len(trainloader)-image_size:\n",
    "                break\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            labels = labels.to(device)\n",
    "            if torch.max(labels).item() == 0:\n",
    "                continue     \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if dp_tec == 'CNN':\n",
    "                inputs = torch.transpose(inputs,1,2)\n",
    "            if dp_tec == 'GRU':\n",
    "                    hidden = hidden.data\n",
    "                    net.zero_grad()\n",
    "                    outputs, hidden = net(inputs.float(), hidden)\n",
    "            else:\n",
    "                # forward + backward + optimizepadded input size per channel: \n",
    "                outputs = net(inputs.float())\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            real_i += 1\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if real_i % 100 == 99:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, real_i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    duracao = datetime.datetime.now() - comeco\n",
    "    print('Finished Training ', dp_tec, ' para ', appliance, ' com duracao de ', duracao)\n",
    "    print('terminou as ', datetime.datetime.now())\n",
    "    net.eval()\n",
    "    return net, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_net(net, appliance, dp_tec):  \n",
    "    onde_salvar_rede = 'BCEwithlogits_'\n",
    "    nome_appliance = appliance.replace(' ', '_')\n",
    "    # onde_salvar_rede = onde_salvar_rede + dp_tec + '_' + nome_appliance + '_' + 'wo_timestamp.pt'\n",
    "    onde_salvar_rede = onde_salvar_rede + dp_tec + '_' + nome_appliance + '_' + 'wo_timestamp_bn_dropout.pt'\n",
    "\n",
    "    \n",
    "    torch.save(net.state_dict(), onde_salvar_rede)\n",
    "    print('rede salva com sucesso em:')\n",
    "    print(onde_salvar_rede)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(path, dp_tec, appliance, image_size, batch_size):\n",
    "    image_size = app_to_image_size[appliance]\n",
    "    net,_,_ = choose_net(dp_tec, image_size, batch_size)\n",
    "    # net = torch.load(path, map_location=device)\n",
    "    net.load_state_dict(torch.load(path))\n",
    "    print(net.eval())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_start_test(appliance, dp_tec, house, is_test=False):\n",
    "    comeco = datetime.datetime.now()\n",
    "    print(\"-----------------------------\")\n",
    "    if is_test == True:\n",
    "        print('comeco do teste para ', appliance, 'utilizando ', dp_tec, 'foi as', comeco)\n",
    "    else:\n",
    "        print('comeco da validacao para casa', str(house), 'do ', appliance, 'utilizando ', dp_tec, 'foi as', comeco)\n",
    "\n",
    "\n",
    "def test_net(net, dfloader, dp_tec, image_size, hidden = False):\n",
    "    comeco_teste = datetime.datetime.now()\n",
    "    print(net.eval())\n",
    "    array_results = []\n",
    "\n",
    "    for i, data in enumerate(dfloader, 0):\n",
    "\n",
    "        if i > len(dfloader)-image_size:\n",
    "            break\n",
    "\n",
    "        if i % 10000 == 9999:\n",
    "            print(i + 1)\n",
    "            print(datetime.datetime.now() - comeco_teste)\n",
    "\n",
    "        inputs, _ = data\n",
    "        if dp_tec == 'CNN':\n",
    "            inputs = torch.transpose(inputs,1,2)\n",
    "        if dp_tec == 'GRU':\n",
    "            hidden = hidden.data\n",
    "            out, hidden = net(inputs.float(), hidden)\n",
    "        else:\n",
    "            out = net(inputs.float())\n",
    "\n",
    "        for element in out:\n",
    "            array_results.append(element.detach().cpu().numpy())\n",
    "\n",
    "    duracao = datetime.datetime.now() - comeco_teste\n",
    "    print('duracao do teste:', duracao)\n",
    "    print(\"-----------------------------\")\n",
    "    return array_results\n",
    "\n",
    "def out_into_df(df, out, image_size):\n",
    "    df['saidas'] = 0.0\n",
    "    array = df.loc[:, 'saidas'].values\n",
    "\n",
    "    for i, element in enumerate(out):\n",
    "        array[i:i+image_size] += element\n",
    "    # array=array/image_size    \n",
    "    df.loc[:, ('saidas')] = array/image_size\n",
    "    return df\n",
    "\n",
    "def save_df_into_csv(df, dp_tec, appliance, house, is_test=False):\n",
    "    if is_test == True:\n",
    "        nome_arquivo = 'saida_teste_'\n",
    "\n",
    "    else:\n",
    "        nome_arquivo = 'saida_casa_' + str(house) + '_'\n",
    "\n",
    "    nome_appliance = appliance.replace(' ', '_')\n",
    "    # nome_arquivo = nome_arquivo + dp_tec + '_' + nome_appliance + '_wo_timestamp.csv'\n",
    "    nome_arquivo = nome_arquivo + dp_tec + '_' + nome_appliance + 'wo_timestamp_bn_dropout.csv'\n",
    "\n",
    "    \n",
    "\n",
    "    df.to_csv(nome_arquivo, sep=',', header=True, index=True)\n",
    "    return True\n",
    "\n",
    "# def apply_threshold(df, dp_tec):\n",
    "#     df = df.copy()\n",
    "#     df.loc[:, 'ligado'] = df['ligado'].astype(int)\n",
    "#     df.loc[:,'saidas'] = df['saidas'].apply(lambda x: x if x > 0 else 0)\n",
    "#     threshold = try_thresholds(df)\n",
    "#     df.loc[:,'saidas'] = df['saidas'].apply(lambda x: 1 if x > threshold else 0)\n",
    "#     return df\n",
    "\n",
    "def open_csv_as_df(dp_tec, appliance, house, is_test=False):\n",
    "    if is_test == True:\n",
    "        nome_arquivo = 'saida_teste_'\n",
    "        # nome_arquivo = 'saida_teste_dropout2_' #TODO tirar dropout\n",
    "    else:\n",
    "        nome_arquivo = 'saida_casa_' + str(house) + '_'\n",
    "        # nome_arquivo = 'saida_dropout_2_casa_' + str(house) + '_' #TODO tirar dropout\n",
    "    nome_appliance = appliance.replace(' ', '_')\n",
    "    nome_arquivo = 'wo_timestamp/'+nome_arquivo + dp_tec + '_' + nome_appliance + '_wo_timestamp.csv'\n",
    "    df = pd.read_csv(nome_arquivo, sep=',')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comecou as  2022-10-26 12:35:19.060878\n",
      "Comeco do aparelho  kettle  foi as  2022-10-26 12:35:19.062266\n",
      "inicio treino: 2013-03-17 19:12:47\n",
      "final treino: 2013-07-29 13:38:29\n",
      "final teste: 2013-08-31 23:59:53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7462/3504317227.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"ligado\"] = df[\"ligado\"].astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kettle  image_size = 48\n",
      "###########################\n",
      "Comeco do treino para  kettle  utilizando a rede  ANN  inicio as  2022-10-26 12:35:30.395741\n",
      "[1,   100] loss: 0.344\n",
      "[1,   200] loss: 0.280\n",
      "[1,   300] loss: 0.288\n",
      "[1,   400] loss: 0.208\n",
      "[1,   500] loss: 0.174\n",
      "[1,   600] loss: 0.150\n",
      "[1,   700] loss: 0.154\n",
      "[1,   800] loss: 0.172\n",
      "[1,   900] loss: 0.135\n",
      "[1,  1000] loss: 0.132\n",
      "[2,   100] loss: 0.174\n",
      "[2,   200] loss: 0.207\n",
      "[2,   300] loss: 0.249\n",
      "[2,   400] loss: 0.168\n",
      "[2,   500] loss: 0.141\n",
      "[2,   600] loss: 0.121\n",
      "[2,   700] loss: 0.111\n",
      "[2,   800] loss: 0.119\n",
      "[2,   900] loss: 0.105\n",
      "[2,  1000] loss: 0.106\n",
      "[3,   100] loss: 0.124\n",
      "[3,   200] loss: 0.153\n",
      "[3,   300] loss: 0.210\n",
      "[3,   400] loss: 0.158\n",
      "[3,   500] loss: 0.110\n",
      "[3,   600] loss: 0.111\n",
      "[3,   700] loss: 0.099\n",
      "[3,   800] loss: 0.111\n",
      "[3,   900] loss: 0.094\n",
      "[3,  1000] loss: 0.098\n",
      "[4,   100] loss: 0.105\n",
      "[4,   200] loss: 0.154\n",
      "[4,   300] loss: 0.214\n",
      "[4,   400] loss: 0.130\n",
      "[4,   500] loss: 0.097\n",
      "[4,   600] loss: 0.100\n",
      "[4,   700] loss: 0.097\n",
      "[4,   800] loss: 0.103\n",
      "[4,   900] loss: 0.085\n",
      "[4,  1000] loss: 0.086\n",
      "[5,   100] loss: 0.107\n",
      "[5,   200] loss: 0.132\n",
      "[5,   300] loss: 0.171\n",
      "[5,   400] loss: 0.121\n",
      "[5,   500] loss: 0.102\n",
      "[5,   600] loss: 0.102\n",
      "[5,   700] loss: 0.086\n",
      "[5,   800] loss: 0.086\n",
      "[5,   900] loss: 0.074\n",
      "[5,  1000] loss: 0.099\n",
      "Finished Training  ANN  para  kettle  com duracao de  0:04:30.486980\n",
      "terminou as  2022-10-26 12:40:00.882804\n",
      "rede salva com sucesso em:\n",
      "BCEwithlogits_ANN_kettle_wo_timestamp_dropout_ultimo.pt\n",
      "-----------------------------\n",
      "comeco do teste para  kettle utilizando  ANN foi as 2022-10-26 12:40:00.999743\n",
      "Net(\n",
      "  (hd1): Linear(in_features=240, out_features=2500, bias=True)\n",
      "  (hd2): Linear(in_features=2500, out_features=2500, bias=True)\n",
      "  (hd3): Linear(in_features=2500, out_features=2500, bias=True)\n",
      "  (hd4): Linear(in_features=2500, out_features=48, bias=True)\n",
      "  (bn): BatchNorm1d(2500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "duracao do teste: 0:00:29.346579\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7462/721123282.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['saidas'] = 0.0\n",
      "/tmp/ipykernel_7462/721123282.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, ('saidas')] = array/image_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "comeco da validacao para casa 1 do  kettle utilizando  ANN foi as 2022-10-26 12:41:00.779372\n",
      "Net(\n",
      "  (hd1): Linear(in_features=240, out_features=2500, bias=True)\n",
      "  (hd2): Linear(in_features=2500, out_features=2500, bias=True)\n",
      "  (hd3): Linear(in_features=2500, out_features=2500, bias=True)\n",
      "  (hd4): Linear(in_features=2500, out_features=48, bias=True)\n",
      "  (bn): BatchNorm1d(2500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "10000\n",
      "0:00:39.132424\n",
      "20000\n",
      "0:01:17.919754\n",
      "30000\n",
      "0:01:57.328716\n",
      "40000\n",
      "0:02:36.371035\n",
      "50000\n",
      "0:03:15.513114\n",
      "60000\n",
      "0:03:54.379253\n",
      "70000\n",
      "0:04:33.663420\n",
      "80000\n",
      "0:05:12.937255\n",
      "90000\n",
      "0:05:51.721678\n",
      "100000\n",
      "0:06:30.361551\n",
      "110000\n",
      "0:07:09.925209\n",
      "duracao do teste: 0:07:11.848927\n",
      "-----------------------------\n",
      "-----------------------------\n",
      "comeco da validacao para casa 2 do  kettle utilizando  ANN foi as 2022-10-26 12:48:58.229025\n",
      "Net(\n",
      "  (hd1): Linear(in_features=240, out_features=2500, bias=True)\n",
      "  (hd2): Linear(in_features=2500, out_features=2500, bias=True)\n",
      "  (hd3): Linear(in_features=2500, out_features=2500, bias=True)\n",
      "  (hd4): Linear(in_features=2500, out_features=48, bias=True)\n",
      "  (bn): BatchNorm1d(2500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "10000\n",
      "0:00:38.809522\n",
      "20000\n",
      "0:01:18.767506\n",
      "30000\n",
      "0:01:57.421422\n",
      "duracao do teste: 0:02:34.632892\n",
      "-----------------------------\n",
      "-----------------------------\n",
      "comeco da validacao para casa 5 do  kettle utilizando  ANN foi as 2022-10-26 12:51:53.291118\n",
      "Net(\n",
      "  (hd1): Linear(in_features=240, out_features=2500, bias=True)\n",
      "  (hd2): Linear(in_features=2500, out_features=2500, bias=True)\n",
      "  (hd3): Linear(in_features=2500, out_features=2500, bias=True)\n",
      "  (hd4): Linear(in_features=2500, out_features=48, bias=True)\n",
      "  (bn): BatchNorm1d(2500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "10000\n",
      "0:00:38.623432\n",
      "duracao do teste: 0:01:00.918269\n",
      "-----------------------------\n",
      "fim do aparelho  kettle as  2022-10-26 12:53:01.871023\n",
      "duracao de  0:17:42.808850\n"
     ]
    }
   ],
   "source": [
    "comeco_total = datetime.datetime.now()\n",
    "print('comecou as ', comeco_total)\n",
    "\n",
    "for i in appliances:\n",
    "    comeco_aparelho = datetime.datetime.now()\n",
    "    \n",
    "    ukdale = DataSet(caminho_ukdale)\n",
    "\n",
    "    #Escolha se deseja escolher uma janela de tempo e o período de inicio e fim:    \n",
    "    set_timewindow = True\n",
    "    startDate = '2013-03-01'\n",
    "    endDate = '2013-09-01'\n",
    "    \n",
    "    #Escolha do eletrodomestico:\n",
    "    appliance = appliances[i]['app']\n",
    "    power = appliances[i]['power']\n",
    "    \n",
    "    if set_timewindow:\n",
    "        ukdale.set_window(start=startDate, end=endDate)\n",
    "    \n",
    "    print('Comeco do aparelho ', appliance , ' foi as ', comeco_aparelho)\n",
    "    #create Dataset\n",
    "    df = create_dataset(ukdale, appliance, power, 1)\n",
    "    \n",
    "    #Desagregacao\n",
    "    train,test = separate_training_from_test(df)\n",
    "\n",
    "    #Batch Size\n",
    "    biggest_window = calculate_batch_size(train)\n",
    "\n",
    "    #Criando Dataset sequencial\n",
    "    image_size = biggest_window + 2\n",
    "    if image_size < 41:\n",
    "        image_size = 42\n",
    "    batch_size = 64\n",
    "    target_cols = 5\n",
    "    \n",
    "    trainloader = create_sequential_dataset(train, image_size, batch_size, target_cols)\n",
    "    \n",
    "    #pro futuro:\n",
    "    print(appliance, ' image_size =', image_size)\n",
    "    \n",
    "    ####running all for all appliances#####\n",
    "    for dp_tec in deep_learning_technique:\n",
    "        \n",
    "        #training\n",
    "        torch.cuda.empty_cache()\n",
    "        net, criterion, optimizer = choose_net(dp_tec, image_size, batch_size)\n",
    "        net.eval()\n",
    "        net, hidden = train_net(net, trainloader, optimizer, criterion, appliance, dp_tec, image_size)\n",
    "        save_net(net, appliance, dp_tec)\n",
    "\n",
    "        #testing\n",
    "        torch.cuda.empty_cache()\n",
    "        print_start_test(appliance, dp_tec, 1, is_test=True)\n",
    "        testloader = create_sequential_dataset(test, image_size, batch_size, target_cols)\n",
    "        out = test_net(net, testloader, dp_tec, image_size, hidden=hidden)\n",
    "        test = out_into_df(test, out, image_size)\n",
    "        save_df_into_csv(test,   dp_tec, appliance, 1, is_test=True)\n",
    "\n",
    "        # resultados em casas nao vistas\n",
    "        for house in homes:\n",
    "            torch.cuda.empty_cache()\n",
    "            if house == 1:\n",
    "                set_timewindow = True\n",
    "                startDate = '2013-09-01'\n",
    "                endDate = '2015-05-01'\n",
    "            else:\n",
    "                set_timewindow = False\n",
    "            ukdale = DataSet(caminho_ukdale)\n",
    "            if set_timewindow:\n",
    "                ukdale.set_window(start=startDate, end=endDate)\n",
    "            if house == 2 and appliance=='air dryer':\n",
    "                pass\n",
    "            df = create_dataset(ukdale, appliance, power, house)\n",
    "            net.eval()\n",
    "\n",
    "            print_start_test(appliance, dp_tec, house)\n",
    "            dfloader = create_sequential_dataset(df, image_size, batch_size, target_cols)\n",
    "            out = test_net(net, dfloader, dp_tec, image_size, hidden=hidden)\n",
    "            df = out_into_df(df, out, image_size)\n",
    "            save_df_into_csv(df, dp_tec, appliance, house)\n",
    "        \n",
    "    print('fim do aparelho ', appliance, 'as ', datetime.datetime.now())\n",
    "    print('duracao de ', datetime.datetime.now() - comeco_aparelho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for house in [2,5]:\n",
    "#     torch.cuda.empty_cache()\n",
    "#     ukdale = DataSet(caminho_ukdale)\n",
    "#     set_timewindow = False\n",
    "#     df = create_dataset(ukdale, appliance, power, house)\n",
    "#     net.eval()\n",
    "\n",
    "#     print_start_test(appliance, dp_tec, house)\n",
    "#     dfloader = create_sequential_dataset(df, image_size, batch_size, target_cols)\n",
    "#     out = test_net(net, dfloader, dp_tec, image_size, hidden=hidden)\n",
    "#     df = out_into_df(df, out, image_size)\n",
    "#     save_df_into_csv(df, dp_tec, appliance, house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testar thresholds e salvar metricas utilizadas\n",
    "\n",
    "# for i in appliances:\n",
    "#         app = appliances[i]['app']\n",
    "#         for dp_tec in deep_learning_technique:\n",
    "#                 print('Resultas para o teste do app:', app, 'usando:', dp_tec)\n",
    "#                 test = open_csv_as_df(dp_tec, app, 1, is_test=True)\n",
    "#                 text = try_thresholds(test)\n",
    "#                 save_results_txt(text, app, dp_tec, house = 1, test = True)\n",
    "\n",
    "#                 for house in homes:\n",
    "#                         print('Resultas para a validacao do app:', app, 'usando:', dp_tec, 'na casa:', house)\n",
    "#                         if house==5 and app=='hair dryer':\n",
    "#                                 pass\n",
    "#                         df = open_csv_as_df(dp_tec, app, house)\n",
    "#                         text = try_thresholds(df)\n",
    "#                         save_results_txt(text, app, dp_tec, house = house, test = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# from matplotlib import rcParams\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# matplotlib.rc('font', size=MEDIUM_SIZE)\n",
    "# matplotlib.rc('axes', titlesize=BIGGER_SIZE)\n",
    "\n",
    "# threshold_df = pd.read_csv('resultados.csv', sep=',')\n",
    "\n",
    "# def apply_threshold(df, threshold):\n",
    "#     df = df.copy()\n",
    "#     df.loc[:, 'ligado'] = df['ligado'].astype(int)\n",
    "#     df.loc[:,'saidas'] = df['saidas'].apply(lambda x: x if x > 0 else 0)\n",
    "#     df.loc[:,'saidas'] = df['saidas'].apply(lambda x: 1 if x > threshold else 0)\n",
    "#     return df\n",
    "\n",
    "# for i in appliances:\n",
    "#         app = appliances[i]['app']\n",
    "#         for dp_tec in deep_learning_technique:\n",
    "#                 print('Resultas para o teste do app:', app, 'usando:', dp_tec)\n",
    "#                 test = open_csv_as_df(dp_tec, app, 1, is_test=True)\n",
    "#                 threshold = threshold_df[(threshold_df.Aparelho.str.lower() == app) & (threshold_df.Casa == 'Teste') &]['threshold']\n",
    "#                 df = apply_threshold(test, threshold)\n",
    "#                 plt.xlabel('Timestamp')\n",
    "#                 plt.plot(test.index, df['ligado'], label = 'ligado')\n",
    "#                 plt.plot(test.index, df['saidas'], label = \"ligado_predicao\")\n",
    "#                 # plt.title(label='Resultados', fontdict= {'fontsize': rcParams['axes.titlesize'],\n",
    "#                 # 'fontweight' : rcParams['axes.titleweight'],\n",
    "#                 # 'verticalalignment': 'baseline',\n",
    "#                 # 'horizontalalignment': 'center'})\n",
    "#                 plt.ylabel('Resultado')\n",
    "#                 plt.legend()\n",
    "#                 plt.show()\n",
    "#                 where_to_save = 'resultado_kettle_test.eps'\n",
    "#                 plt.savefig(where_to_save, format='eps')\n",
    "\n",
    "\n",
    "\n",
    "#                 for house in homes:\n",
    "#                         print('Resultas para a validacao do app:', app, 'usando:', dp_tec, 'na casa:', house)\n",
    "#                         if house==5 and app=='hair dryer':\n",
    "#                                 pass\n",
    "#                         df = open_csv_as_df(dp_tec, app, house)\n",
    "#                         text = try_thresholds(df)\n",
    "#                         save_results_txt(text, app, dp_tec, house = house, test = False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #se quiser pular o treino utilizando a rede do ANN\n",
    "# path='BCEwithlogits_ANN_kettle_v2.pt'\n",
    "# app = appliances[0]['app']\n",
    "# dp_tec=deep_learning_technique[0]\n",
    "# image_size = app_to_image_size[app]\n",
    "# batch_size=64\n",
    "# net = load_net(path, dp_tec, appliance, image_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# from matplotlib import rcParams\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # matplotlib.rc('font', size=MEDIUM_SIZE)\n",
    "# # matplotlib.rc('axes', titlesize=BIGGER_SIZE)\n",
    "\n",
    "# threshold_df = pd.read_csv('resultados.csv', sep=',')\n",
    "\n",
    "# def apply_threshold(df, threshold):\n",
    "#     df = df.copy()\n",
    "#     df.loc[:, 'ligado'] = df['ligado'].astype(int)\n",
    "#     df.loc[:, 'saidas'] = df['saidas'].astype(int)\n",
    "#     # df.loc[:,'saidas'] = df['saidas'].apply(lambda x: x if x > 0 else 0)\n",
    "#     print(threshold)\n",
    "#     df.loc[:,'saidas'] = df['saidas'].apply(lambda x: 1.02 if x > threshold else 0)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# app = 'kettle'\n",
    "# dp_tec = 'ANN'\n",
    "# test = open_csv_as_df(dp_tec, app, 1, is_test=True)\n",
    "# test = test.set_index('time')\n",
    "# threshold = threshold_df[(threshold_df.Aparelho.str.lower() == app) & (threshold_df.Casa == 'Teste') & (threshold_df.Ferramenta == dp_tec)]['Threshold'][0]\n",
    "# df = apply_threshold(test, threshold)\n",
    "# plt.xlabel('Timestamp')\n",
    "# plt.set_xticklabels(df,time)\n",
    "# plt.plot(df.index, df['ligado'], label = 'ligado')\n",
    "# plt.plot(df.index, df['saidas'], label = \"ligado_predicao\")\n",
    "# # plt.title(label='Resultados', fontdict= {'fontsize': rcParams['axes.titlesize'],\n",
    "# # 'fontweight' : rcParams['axes.titleweight'],\n",
    "# # 'verticalalignment': 'baseline',\n",
    "# # 'horizontalalignment': 'center'})\n",
    "# plt.ylabel('Resultado')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# where_to_save = 'resultado_kettle_test.eps'\n",
    "# plt.savefig(where_to_save, format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d0578ba25df9157e26c0a1c62fddb67b0994bec13c5b6da606c999bba7b0821"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}