{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [],
   "source": [
    "appliances = {\n",
    "        # 0: {'app': 'kettle', 'power': 2000},\n",
    "        1: {'app': 'toaster', 'power': 1200},\n",
    "        2: {'app': 'dish washer', 'power': 2000},\n",
    "        # 0: {'app': 'kettle', 'power': 2300},\n",
    "        # 1: {'app': 'toaster', 'power': 1500},\n",
    "        # 2: {'app': 'dish washer', 'power': 2300},\n",
    "        # 3: {'app': 'microwave', 'power':1500}\n",
    "        # 4: {'app': 'hair dryer', 'power': 1000},\n",
    "        # 5: {'app': 'audio amplifier', 'power': 20},\n",
    "    }\n",
    "\n",
    "#air dryer nao tem na casa 2?\n",
    "#tirar hair dryer e audio amplifier!!!\n",
    "#falta testar onde fica o dropout da ann!!\n",
    "\n",
    "machine_learning_technique = ['logisticRegression', 'KNN', 'SVM', 'SGD', 'randomForest', 'perceptron']\n",
    "\n",
    "homes = [1]#,2,5]\n",
    "\n",
    "#esse dict soh serve pra dar load na rede\n",
    "app_to_image_size = {\n",
    "    'kettle': 48,\n",
    "    'toaster': 64,\n",
    "    'audio amplifier': 0,\n",
    "    'dish washer': 166,\n",
    "    'hair dryer': 27,\n",
    "    'microwave': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import nilmtk\n",
    "from nilmtk import DataSet, MeterGroup\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression, SGDClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "plt.style.use('ggplot')\n",
    "rcParams['figure.figsize'] = (20, 8)\n",
    "caminho_ukdale = 'ukdale.h5'\n",
    "\n",
    "# Lendo Dataset do Ukdale\n",
    "ukdale = DataSet(caminho_ukdale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(ukdale, appliance, power, house):\n",
    "    # Lendo eletrodomesticos da Casa House\n",
    "    elec = ukdale.buildings[house].elec\n",
    "\n",
    "    #Criando dataset do mains\n",
    "    main_df = next(elec.mains().load())\n",
    "    # print(main_df.head())\n",
    "    main_df.columns = [' '.join(col).strip() for col in main_df.columns.values]\n",
    "    main_df = main_df.rename(columns={'power apparent': 'pwr_app_mains', 'power active': 'pwr_act_mains', 'voltage': 'voltage_mains'})\n",
    "    main_df = main_df.sort_index()\n",
    "    \n",
    "    appliance_df = next(elec[appliance].load())\n",
    "    appliance_df.columns = ['pwr_act_' + appliance]\n",
    "    appliance_df = appliance_df.sort_index()\n",
    "    \n",
    "    #-------------------Upsample do main_df--------------------------#\n",
    "    appliance_df = appliance_df[appliance_df.index >= main_df.index[0]]\n",
    "\n",
    "    main_df = main_df.resample('6S', offset='5s').mean().interpolate()\n",
    "    main_df = main_df[main_df.index >= main_df.index[1]]\n",
    "    main_df = main_df[main_df.index <= appliance_df.index[-1]]\n",
    "\n",
    "    #------------------Resampling appliance_df-----------------------#\n",
    "    appliance_df = appliance_df.resample('6S', offset='5s').mean().interpolate()\n",
    "\n",
    "    #-------------------Merging dataframes---------------------------#\n",
    "    merged_df = main_df.merge(appliance_df, left_index=True, right_index=True)\n",
    "    merged_df['time'] = merged_df.index\n",
    "    merged_df['time'] = merged_df['time'].apply(lambda x: x.tz_localize(None))\n",
    "    merged_df.index = merged_df['time']\n",
    "    merged_df['dia_semana'] = merged_df.time.dt.dayofweek\n",
    "    merged_df['mes'] = merged_df.time.dt.month\n",
    "    merged_df['hora'] = merged_df.time.dt.hour\n",
    "    merged_df['timestamp'] = merged_df['time'].apply(lambda x: x.timestamp())\n",
    "    merged_df = merged_df.drop(columns=['time'])\n",
    "\n",
    "    #---------------Criando coluna classificatoria-------------------#\n",
    "    merged_df['ligado'] = merged_df['pwr_act_' + appliance] > power\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_training_from_test(df):\n",
    "    #Fazendo a divisao entre treino e teste de forma que 1 milhao de pontos sejam utilizados para treinar\n",
    "    train_test_divide = df.index[int(0.8*len(df))]\n",
    "\n",
    "    print(\"inicio treino: \" + str(df.index[0]))\n",
    "    print(\"final treino: \" + str(train_test_divide))\n",
    "    print(\"final teste: \" + str(df.index[-1]))\n",
    "    \n",
    "    train = df[df.index <= train_test_divide]\n",
    "    test = df[df.index > train_test_divide]\n",
    "    \n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequential_dataset(df):\n",
    "    df = df[['voltage_mains', 'pwr_app_mains', 'pwr_act_mains', 'dia_semana', 'hora', 'ligado']]\n",
    "    df_scaled = pd.DataFrame(np.append(\n",
    "        preprocessing.scale(df[['voltage_mains', 'pwr_app_mains', 'pwr_act_mains', 'dia_semana', 'hora']]), df[['ligado']].values, axis=1),\n",
    "                                index=df.index, columns=df.columns)\n",
    "\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "def choose_net(ml_tec, appliance):\n",
    "    if ml_tec == 'logisticRegression':\n",
    "        if appliance == 'kettle':\n",
    "            return LogisticRegression(class_weight={0: 0.2, 1: 0.8}, multi_class='multinomial', penalty='none', solver='lbfgs', n_jobs=-1) #kettle\n",
    "        if appliance == 'toaster':\n",
    "            return LogisticRegression(class_weight={0: 0.15, 1: 0.85}, multi_class='multinomial', penalty='none', solver='newton-cg', n_jobs=-1) #toaster\n",
    "        if appliance == 'dish washer':\n",
    "            return LogisticRegression(class_weight={0: 0.15, 1: 0.85}, multi_class='multinomial', penalty='none', solver='newton-cg', n_jobs=-1) #dishWasher\n",
    "\n",
    "\n",
    "    if ml_tec == 'KNN':  # demorado\n",
    "        if appliance == 'kettle':\n",
    "            return KNeighborsClassifier(weights='distance', p=3, n_neighbors=9, metric='minkowski', leaf_size=40, algorithm='kd_tree', n_jobs=-1) #kettle\n",
    "        if appliance == 'toaster':\n",
    "            return KNeighborsClassifier(weights='distance', p=4, n_neighbors=11, metric='manhattan', leaf_size=50, algorithm='ball_tree', n_jobs=-1) #toaster\n",
    "        if appliance == 'dish washer':\n",
    "            return KNeighborsClassifier(weights='uniform', p=5, n_neighbors=11, metric='minkowski', leaf_size=50, algorithm='kd_tree', n_jobs=-1) #dishWasher 0.725420462724261\n",
    "\n",
    "\n",
    "    if ml_tec == 'SVM':\n",
    "        if appliance == 'kettle':\n",
    "            return LinearSVC(penalty='l2', multi_class='crammer_singer', loss='squared_hinge', dual=True, class_weight={0: 0.25, 1: 0.75}) #kettle\n",
    "        if appliance == 'toaster':\n",
    "            return LinearSVC(penalty='l2', multi_class='crammer_singer', loss='squared_hinge', dual=True, class_weight={0: 0.2, 1: 0.8}) #toaster\n",
    "        if appliance == 'dish washer':\n",
    "            return LinearSVC(penalty='l1', multi_class='crammer_singer', loss='squared_hinge', dual=False, class_weight={0: 0.2, 1: 0.8}) #dishWasher\n",
    "\n",
    "    if ml_tec == 'SGD':\n",
    "        if appliance == 'kettle':\n",
    "            return SGDClassifier(penalty=None, loss='hinge', l1_ratio=0.1, class_weight= {0: 0.15, 1: 0.85}, n_jobs=-1)\n",
    "        if appliance == 'toaster':\n",
    "            return SGDClassifier(penalty=None, loss='modified_huber', class_weight= {0: 0.15, 1: 0.85}, n_jobs=-1) #toaster\n",
    "        if appliance == 'dish washer':\n",
    "            return SGDClassifier(penalty=None, loss='hinge', class_weight= {0: 0.15, 1: 0.85}, n_jobs=-1) #dishWasher\n",
    "\n",
    "    if ml_tec == 'randomForest': # demorado\n",
    "        if appliance == 'kettle':\n",
    "            return RandomForestClassifier(criterion='gini', max_features='sqrt', n_estimators=300, class_weight={0: 0.2, 1: 0.8}, n_jobs=-1) #kettle\n",
    "        if appliance == 'toaster':\n",
    "            return RandomForestClassifier(criterion='entropy', max_features='sqrt', n_estimators=300, class_weight={0: 0.3, 1: 0.7}, n_jobs=-1) #toaster\n",
    "        if appliance == 'dish washer':\n",
    "            return RandomForestClassifier(criterion='entropy', max_features='sqrt', n_estimators=300, class_weight={0: 0.2, 1: 0.8}, n_jobs=-1) #dishWasher\n",
    "\n",
    "    if ml_tec == 'perceptron':\n",
    "        if appliance == 'kettle':\n",
    "            return Perceptron(n_jobs=-1, penalty='elasticnet', l1_ratio=0.15, class_weight={0: 0.3, 1: 0.7}) #kettle\n",
    "        if appliance == 'toaster':\n",
    "            return Perceptron(n_jobs=-1, penalty='l1', l1_ratio=0.1, class_weight={0: 0.25, 1: 0.75}) #toaster\n",
    "        if appliance == 'dish washer':\n",
    "            return Perceptron(n_jobs=-1, penalty=None, l1_ratio=0.1, class_weight={0: 0.15, 1: 0.85}) #dishWasher\n",
    "\n",
    "# def parameter_grid_svm():\n",
    "#     random_grid = {\n",
    "#                'penalty' : ['l1','l2'],\n",
    "#                'loss' : ['hinge', 'squared_hinge'],\n",
    "#                'dual': [False],# 'False'],\n",
    "#                'multi_class' : ['crammer_singer'],#['ovr','crammer_singer'],\n",
    "#                'class_weight': [{0:.2, 1:.8}, {0:.25, 1:.75}],#[{0:.15, 1:.85}, {0:.2, 1:.8}, {0:.25, 1:.75}, {0:.3, 1:.7}]# {0:.35, 1:.65}] \n",
    "#                }\n",
    "#     return random_grid\n",
    "\n",
    "# def parameter_grid_logistic_regression():\n",
    "#     random_grid = dict()\n",
    "#     random_grid['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "#     random_grid['penalty'] = ['none']#, 'l1', 'l2', 'elasticnet']\n",
    "#     # random_grid['C'] = [0.005, 0.05, 0.5, 1, 5]\n",
    "#     random_grid['multi_class'] = ['multinomial']#, 'ovr']\n",
    "#     random_grid['class_weight'] = [{0:.15, 1:.85}, {0:.2, 1:.8}, {0:.25, 1:.75}] \n",
    "#     return random_grid\n",
    "\n",
    "# def parameter_grid_knn():\n",
    "#     random_grid = {'n_neighbors' : [11], #[9,11,15],#[5,7,9,11,15],\n",
    "#                'weights' : ['uniform', 'distance'],\n",
    "#                'algorithm': ['ball_tree', 'kd_tree'],#['auto', 'ball_tree', 'kd_tree'],\n",
    "#                'metric' : ['minkowski','euclidean','manhattan'],\n",
    "#                'p': [3, 4, 5],#[2, 3, 4, 5],\n",
    "#                'leaf_size': [40, 50],# 60],#[30, 40, 50, 60]\n",
    "#                }\n",
    "#     return random_grid\n",
    "\n",
    "# def parameter_grid():\n",
    "#     random_grid = {\n",
    "#                'penalty' : [None],#['l1','l2', 'elasticnet', None],\n",
    "#                'loss' : ['hinge'],#['hinge', 'log_loss', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "#             #    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "#             #    'l1_ratio': [0.05, 0.1, 0.15, 0.2],\n",
    "#             #    'shuffle': [True],# False],\n",
    "#             #    'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "#             #    'early_stopping': [True, False],\n",
    "#                'class_weight': [{0:.15, 1:.85}, {0:.2, 1:.8}, {0:.25, 1:.75}, {0:.3, 1:.7}, {0:.35, 1:.65}] \n",
    "#                }\n",
    "#     return random_grid\n",
    "\n",
    "# def parameter_grid_rf():\n",
    "#     random_grid = {\n",
    "#                'criterion' : ['gini', 'entropy', 'log_loss'],\n",
    "#                'max_features' : ['sqrt', None],# 'log2', None],\n",
    "#                'class_weight': [{0:.2, 1:.8}, {0:.3, 1:.7}],# [{0:.15, 1:.85}, {0:.2, 1:.8}, {0:.25, 1:.75}, {0:.3, 1:.7}],# {0:.35, 1:.65}],\n",
    "#                'n_estimators': [300],#[50,100,200,300]\n",
    "#                }\n",
    "#     return random_grid\n",
    "\n",
    "# def parameter_grid_percebetron():\n",
    "#     random_grid = {\n",
    "#                'penalty' : ['l1','l2', 'elasticnet', None],\n",
    "#             #    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "#                'l1_ratio': [0.1, 0.15],#[0.05, 0.1, 0.15, 0.2],\n",
    "#             #    'shuffle': [True, False],\n",
    "#             #    'early_stopping': [True, False],\n",
    "#                'class_weight': [{0:.15, 1:.85}, {0:.2, 1:.8}, {0:.25, 1:.75}, {0:.3, 1:.7}, {0:.35, 1:.65}] \n",
    "#                }\n",
    "#     return random_grid\n",
    "\n",
    "def randomize_search(estimator, random_grid, dataset):\n",
    "    # cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    model_random = RandomizedSearchCV(estimator = estimator,param_distributions = random_grid,\n",
    "               n_iter = 10, cv = 5, scoring='f1', verbose=2, random_state=8, n_jobs = -1)\n",
    "    model_grid = GridSearchCV(estimator = estimator,param_grid = random_grid, cv = 5, scoring='f1', verbose=3, n_jobs = -1)\n",
    "    return model_grid.fit(dataset[['voltage_mains', 'pwr_app_mains', 'pwr_act_mains', 'dia_semana', 'hora']], dataset[['ligado']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, dataset, appliance):\n",
    "    comeco = datetime.datetime.now()\n",
    "    print('###########################')\n",
    "    print('Comeco do treino para ', appliance, ' utilizando a rede ', ml_tec, ' inicio as ', comeco)\n",
    "\n",
    "    net.fit(dataset[['voltage_mains', 'pwr_app_mains', 'pwr_act_mains', 'dia_semana', 'hora']], dataset[['ligado']])\n",
    "\n",
    "    duracao = datetime.datetime.now() - comeco\n",
    "    print('Finished Training ', ml_tec, ' para ', appliance, ' com duracao de ', duracao)\n",
    "    print('terminou as ', datetime.datetime.now())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_start_test(appliance, ml_tec, house, is_test=False):\n",
    "    comeco = datetime.datetime.now()\n",
    "    print(\"-----------------------------\")\n",
    "    if is_test == True:\n",
    "        print('comeco do teste para ', appliance, 'utilizando ', ml_tec, 'foi as', comeco)\n",
    "    else:\n",
    "        print('comeco da validacao para casa', str(house), 'do ', appliance, 'utilizando ', ml_tec, 'foi as', comeco)\n",
    "\n",
    "\n",
    "def test_net(net, dataset):\n",
    "    comeco_teste = datetime.datetime.now()\n",
    "\n",
    "    array_results = net.predict(dataset[['voltage_mains', 'pwr_app_mains', 'pwr_act_mains', 'dia_semana', 'hora']])\n",
    "\n",
    "    duracao = datetime.datetime.now() - comeco_teste\n",
    "    print('duracao do teste:', duracao)\n",
    "    print(\"-----------------------------\")\n",
    "    return array_results\n",
    "\n",
    "def out_into_df(df, out):\n",
    "    df['saidas'] = out\n",
    "    return df\n",
    "\n",
    "def save_df_into_csv(df, ml_tec, appliance, house, is_test=False):\n",
    "    if is_test == True:\n",
    "        nome_arquivo = 'saida_teste_' #TODO\n",
    "\n",
    "    else:\n",
    "        nome_arquivo = 'saida_casa_' + str(house) + '_'\n",
    "\n",
    "    nome_appliance = appliance.replace(' ', '_')\n",
    "    nome_arquivo = 'results/csv/'+nome_arquivo + ml_tec + '_' + nome_appliance + '_v2.csv'\n",
    "\n",
    "    df.to_csv(nome_arquivo, sep=',', header=True, index=True)\n",
    "    return True\n",
    "\n",
    "def open_csv_as_df(ml_tec, appliance, house, is_test=False):\n",
    "    if is_test == True:\n",
    "        nome_arquivo = 'saida_teste_'\n",
    "    else:\n",
    "        nome_arquivo = 'saida_casa_' + str(house) + '_'\n",
    "    nome_appliance = appliance.replace(' ', '_')\n",
    "    nome_arquivo = nome_arquivo + ml_tec + '_' + nome_appliance + '.csv'\n",
    "    df = pd.read_csv(nome_arquivo, sep=',')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_txt(text, appliance, ml_tec, house = 1, test = False):  \n",
    "    file_name = 'results_'\n",
    "    nome_appliance = appliance.replace(' ', '_')\n",
    "    if test == True:\n",
    "        file_name = file_name + '_teste_'\n",
    "    file_name = file_name + 'casa_' + str(house) + '_'\n",
    "    file_name = file_name + ml_tec + '_' + nome_appliance + '.txt'\n",
    "    f = open(file_name, 'w+')  # open file in write mode\n",
    "    f.write(text)\n",
    "    f.close()\n",
    "    print('resultados salvos com sucesso em:', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score  \n",
    "\n",
    "def calculate_metrics(dataset):\n",
    "    dataset[\"ligado\"] = dataset[\"ligado\"].astype(int)\n",
    "    y = dataset.loc[:,('ligado')].values\n",
    "    y_pred = dataset.loc[:,('saidas')].values\n",
    "\n",
    "\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "\n",
    "    \n",
    "    print(f1, accuracy, recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comecou as  2023-04-12 19:05:01.175667\n",
      "Comeco do aparelho  toaster  foi as  2023-04-12 19:05:01.177837\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4383/2433151644.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Comeco do aparelho '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappliance\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m' foi as '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomeco_aparelho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#create Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mukdale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappliance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#Desagregacao\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4383/281956301.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(ukdale, appliance, power, house)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#Criando dataset do mains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# print(main_df.head())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nilmtk/nilmtk/datastore/hdfdatastore.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, key, columns, sections, n_look_ahead_rows, chunksize, verbose)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_as_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     if str(e) == (\"'NoneType' object has no attribute \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mselect_as_coordinates\u001b[0;34m(self, key, where, start, stop)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtbl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can only read_coordinates with a table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m     def select_column(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_coordinates\u001b[0;34m(self, where, start, stop)\u001b[0m\n\u001b[1;32m   4209\u001b[0m         \u001b[0;31m# create the selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4210\u001b[0m         \u001b[0mselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4211\u001b[0;31m         \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4213\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mselect_coords\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcondition\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5330\u001b[0;31m             return self.table.table.get_where_list(\n\u001b[0m\u001b[1;32m   5331\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5332\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tables/table.py\u001b[0m in \u001b[0;36mget_where_list\u001b[0;34m(self, condition, condvars, sort, start, stop, step)\u001b[0m\n\u001b[1;32m   1606\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_check_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m         coords = [p.nrow for p in\n\u001b[0m\u001b[1;32m   1609\u001b[0m                   self._where(condition, condvars, start, stop, step)]\n\u001b[1;32m   1610\u001b[0m         \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSizeType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tables/table.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1606\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_check_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m         coords = [p.nrow for p in\n\u001b[0m\u001b[1;32m   1609\u001b[0m                   self._where(condition, condvars, start, stop, step)]\n\u001b[1;32m   1610\u001b[0m         \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSizeType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "comeco_total = datetime.datetime.now()\n",
    "print('comecou as ', comeco_total)\n",
    "\n",
    "results_df = pd.DataFrame(columns=['app', 'tec', 'house', 'recall', 'accuracy', 'precision', 'f1', 'TP', 'TN', 'FP', 'FN', 'P', 'N'])\n",
    "for i in appliances:\n",
    "    comeco_aparelho = datetime.datetime.now()\n",
    "    \n",
    "    ukdale = DataSet(caminho_ukdale)\n",
    "\n",
    "    #Escolha se deseja escolher uma janela de tempo e o período de inicio e fim:    \n",
    "    set_timewindow = True\n",
    "    startDate = '2013-03-01'\n",
    "    endDate = '2013-09-01'\n",
    "    \n",
    "    #Escolha do eletrodomestico:\n",
    "    appliance = appliances[i]['app']\n",
    "    power = appliances[i]['power']\n",
    "    \n",
    "    if set_timewindow:\n",
    "        ukdale.set_window(start=startDate, end=endDate)\n",
    "    \n",
    "    print('Comeco do aparelho ', appliance , ' foi as ', comeco_aparelho)\n",
    "    #create Dataset\n",
    "    df = create_dataset(ukdale, appliance, power, 1)\n",
    "    \n",
    "    #Desagregacao\n",
    "    train,test = separate_training_from_test(df)\n",
    "    \n",
    "    trainloader = create_sequential_dataset(train)\n",
    "    # trainloader = create_sequential_dataset(df)\n",
    "\n",
    "    ####running all for all appliances#####\n",
    "    for ml_tec in machine_learning_technique:\n",
    "        \n",
    "        #training\n",
    "        net = choose_net(ml_tec, appliance)\n",
    "        net = train_net(net, trainloader, appliance)\n",
    "\n",
    "        #validating\n",
    "        # random_grid = parameter_grid()\n",
    "        # randomized_net = randomize_search(net, random_grid, trainloader)\n",
    "        # print(randomized_net.best_params_)\n",
    "        # print(randomized_net.best_score_)\n",
    "\n",
    "        # testing\n",
    "        print_start_test(appliance, ml_tec, 1, is_test=True)\n",
    "        testloader = create_sequential_dataset(test)\n",
    "        out = test_net(net, testloader)\n",
    "        test = out_into_df(test, out)\n",
    "        save_df_into_csv(test, ml_tec, appliance, 1, is_test=True)\n",
    "\n",
    "        # # resultados em casas nao vistas\n",
    "        for house in homes:\n",
    "            if house == 1:\n",
    "                set_timewindow = True\n",
    "                startDate = '2013-09-01'\n",
    "                endDate = '2015-05-01'\n",
    "            else:\n",
    "                set_timewindow = False\n",
    "            ukdale = DataSet(caminho_ukdale)\n",
    "            if set_timewindow:\n",
    "                ukdale.set_window(start=startDate, end=endDate)\n",
    "            df = create_dataset(ukdale, appliance, power, house)\n",
    "\n",
    "            print_start_test(appliance, ml_tec, house)\n",
    "            dfloader = create_sequential_dataset(df)\n",
    "            out = test_net(net, dfloader)\n",
    "            df = out_into_df(df, out)\n",
    "            save_df_into_csv(df, ml_tec, appliance, house)\n",
    "        \n",
    "print('fim do aparelho ', appliance, 'as ', datetime.datetime.now())\n",
    "print('duracao de ', datetime.datetime.now() - comeco_aparelho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': {0: 0.15, 1: 0.85}, 'loss': 'hinge', 'penalty': None}\n",
      "0.6047103715335518\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6094313157178869"
      ]
     },
     "execution_count": 945,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "0.6094313157178869"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d0578ba25df9157e26c0a1c62fddb67b0994bec13c5b6da606c999bba7b0821"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}